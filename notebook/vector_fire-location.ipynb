{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Describe the analysis and why useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard packages\n",
    "import os                           # Reproducible file names\n",
    "import warnings                     # View warnings\n",
    "\n",
    "# Import third part packages\n",
    "import earthpy as et                # File organization\n",
    "import geopandas as gpd             # Enables work in geodataframes\n",
    "import geoviews as gv               # Enables work with geographic data\n",
    "import holoviews as hv              # For use with interactive plotting\n",
    "import hvplot.pandas                # Plotting maps and plots\n",
    "import pandas as pd                 # Work with dataframes\n",
    "import pyogrio                       # Help with import of geodatabase\n",
    "\n",
    "# warning.simplefilter('ignore')    # Suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watershed Dataset Description\n",
    "\n",
    "The watershed data used in this analysis comes from the [USGS Watershed Boundary Dataset (WBD)](https://www.usgs.gov/national-hydrography/watershed-boundary-dataset) for the United States. All watersheds in this dataset have gauges for stream monitoring. The data used herein are the **HUC-2 Watersheds (Regional Scale)**. \n",
    "\n",
    "Describe the dataset and add citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download watershed data from the WBD Website\n",
    "# Data is a geodatabase of watersheds from the entire nation\n",
    "# Note, to download large data, may need to change machine type of codespace\n",
    "wbd_url = (\n",
    "    \"https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/WBD\"\n",
    "    \"/National/GDB/WBD_National_GDB.zip\")\n",
    "\n",
    "wbd_dir = et.data.get_data(url=wbd_url)    # Path to downloaded file directory\n",
    "wbd_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 2-digit HU layer\n",
    "# Call the GDB file\n",
    "wbd_path = os.path.join(wbd_dir, 'WBD_National_GDB.gdb')\n",
    "\n",
    "# Read in the GDB and layer of interest (regional watersheds - HU2)\n",
    "wbd_hu2_gdf = gpd.read_file(wbd_path, layer='WBDHU2', from_disk=True)\n",
    "wbd_hu2_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test plot to demonstrate successful import of data\n",
    "\n",
    "wbd_hu2_gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wildfire Dataset Description\n",
    "\n",
    "The wildfire data used in this analysis comes from the [USDA's wildfire occurance data for the United States, 1992-2020](https://www.fs.usda.gov/rds/archive/Catalog/RDS-2013-0009.6). This is the fifth version of the dataset and was generated from reporting systems of federal, state, and local fire organizations.\n",
    "\n",
    "Data Citation: \n",
    "Short, Karen C. 2022. Spatial wildfire occurrence data for the United States, 1992-2020 [FPA_FOD_20221014]. 6th Edition. Fort Collins, CO: Forest Service Research Data Archive. https://doi.org/10.2737/RDS-2013-0009.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download wildfire occurance data from geodatabase\n",
    "\n",
    "fire_url = (\n",
    "    \"https://www.fs.usda.gov/rds/archive/products/RDS-2013-0009.6\"\n",
    "    \"/RDS-2013-0009.6_Data_Format2_GDB.zip\"\n",
    ")\n",
    "\n",
    "fire_dir = et.data.get_data(url=fire_url)   # Path to downloaded fire directory\n",
    "fire_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import fires layer from geodatabase (cashing data)\n",
    "# Put import in an if statement to check if data already downloaded\n",
    "fire_path = os.path.join(fire_dir, 'Data','FPA_FOD_20221014.gdb')\n",
    "if not 'fire_gdf' in globals():\n",
    "    print('fire_gdf does not exist. Loading...')\n",
    "    fire_gdf = pyogrio.read_dataframe(fire_path, layer='Fires')\n",
    "\n",
    "fire_gdf.head()                     # Prints only first few lines of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the data (Lat/Long already assigned to geometry)\n",
    "# Create an updated geodatabase, populating specific variables\n",
    "# [[]] Two sets, one to search and one to define a list\n",
    "fire_clean_gdf = (\n",
    "    fire_gdf\n",
    "    [['FOD_ID', 'DISCOVERY_DATE', 'FIRE_SIZE', 'geometry']]\n",
    "    .set_index('FOD_ID')\n",
    ")\n",
    "\n",
    "# Convert the existing date to a datetime format\n",
    "fire_clean_gdf.DISCOVERY_DATE = pd.to_datetime(fire_clean_gdf.DISCOVERY_DATE)\n",
    "\n",
    "# Reproject dataframe to match the CRS of watershed boundary\n",
    "print('Geodetic CRS before reprojection: ' + str(fire_clean_gdf.crs))\n",
    "fire_clean_gdf = fire_clean_gdf.to_crs(wbd_hu2_gdf.crs)\n",
    "print('Geodetic CRS after reprojection: ' + str(fire_clean_gdf.crs))\n",
    "\n",
    "fire_clean_gdf                  # Use .info() to see data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatially join the watershed with the fire history geodataframes\n",
    "fire_region_gdf = (\n",
    "    wbd_hu2_gdf[['name', 'geometry']]\n",
    "    .sjoin(fire_clean_gdf, how='inner', predicate='intersects')\n",
    ")\n",
    "\n",
    "# Calculate max fire size for each year in watershed combination\n",
    "fire_region_gdf = (fire_region_gdf\n",
    "    .groupby(['name', fire_region_gdf.DISCOVERY_DATE.dt.year])\n",
    "    .agg(\n",
    "        fire_size=('FIRE_SIZE','max'),          # New name = old, how\n",
    "        num_fires=('index_right', 'count'))     # New name = old, how\n",
    ")\n",
    "\n",
    "print('Total number of fires: ' + str(fire_region_gdf.num_fires.sum()))\n",
    "fire_region_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the area of each watershed (use Albers Equal Area Projection)\n",
    "wbd_hu2_gdf['area_ha'] = (        # Add an area column to gdf\n",
    "    wbd_hu2_gdf.to_crs(9822)      # Convert new temp gdf to Albers epsg\n",
    "    .area/10000/1000000           # Calculate the watershed area (millions ha)\n",
    ")                                 # Note 1 hectare is 10,000 sq.m.\n",
    "\n",
    "# Calculate total number of fires in each watershed region\n",
    "fire_density_gdf = (fire_region_gdf\n",
    "    .reset_index()              # Reset because name is currently in index\n",
    "    [['name', 'num_fires']]\n",
    "    .groupby('name')            # Group by the name of the region\n",
    "    .sum()                      # Sum of the number of fires \n",
    "    .join(wbd_hu2_gdf.set_index('name'))   # Join wbd to fire_density gdf\n",
    "    [['num_fires', 'area_ha']]             # Add area and geomety back in\n",
    ")\n",
    "\n",
    "# Calculate fire density (number of fires per area)\n",
    "fire_density_gdf['fire_density_per_ha'] = (\n",
    "    fire_density_gdf.num_fires / fire_density_gdf.area_ha\n",
    ")\n",
    "\n",
    "# Print the index\n",
    "fire_density_gdf.fire_density_per_ha"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
